--- ./include/ck/ck.hpp.orig	2023-12-07 18:33:34.435503872 +0000
+++ ./include/ck/ck.hpp	2023-12-07 18:45:11.005502374 +0000
@@ -30,8 +30,8 @@
 // buffer resource
 #ifndef __HIP_DEVICE_COMPILE__ // for host code
 #define CK_BUFFER_RESOURCE_3RD_DWORD -1
-#elif defined(__gfx803__) || defined(__gfx900__) || defined(__gfx906__) || defined(__gfx908__) || \
-    defined(__gfx90a__) || defined(__gfx940__) // for GPU code
+#elif defined(__gfx803__) || defined(__gfx900__) || defined(__gfx902__) || defined(__gfx906__) || \
+    defined(__gfx908__) || defined(__gfx90a__) || defined(__gfx940__) // for GPU code
 #define CK_BUFFER_RESOURCE_3RD_DWORD 0x00020000
 #elif defined(__gfx1030__) // for GPU code
 #define CK_BUFFER_RESOURCE_3RD_DWORD 0x31014000
@@ -41,7 +41,7 @@
 
 // FMA instruction
 #ifndef __HIP_DEVICE_COMPILE__                   // for host code, define nothing
-#elif defined(__gfx803__) || defined(__gfx900__) // for GPU code
+#elif defined(__gfx803__) || defined(__gfx900__) || defined(__gfx902__) // for GPU code
 #define CK_USE_AMD_V_MAC_F32
 #elif defined(__gfx906__) || defined(__gfx908__) || defined(__gfx90a__) || defined(__gfx1030__) || \
     defined(__gfx940__) // for GPU code
--- ./include/ck/utility/amd_inline_asm.hpp.orig	2023-12-07 23:57:17.118795409 +0000
+++ ./include/ck/utility/amd_inline_asm.hpp	2023-12-08 10:41:38.469904977 +0000
@@ -6,6 +6,9 @@
 
 #include "data_type.hpp"
 #include "c_style_pointer_cast.hpp"
+#if (!defined(CK_USE_AMD_V_FMAC_F32) || !defined(CK_USE_AMD_V_DOT4_I32_I8))
+#include <omp.h>
+#endif
 
 // TODO: deprecate all amd_assembly_outer_product_xxx
 
@@ -15,12 +18,21 @@
 // c1 += inner_product(a, b1)
 __device__ void amd_assembly_outer_product_1x2(float a, float b0, float b1, float& c0, float& c1)
 {
+#if defined(CK_USE_AMD_V_FMAC_F32)
     asm volatile("\n \
             v_fmac_f32 %0, %2, %3 \n \
             v_fmac_f32 %1, %2, %4 \n \
             "
                  : "=v"(c0), "=v"(c1)
                  : "v"(a), "v"(b0), "v"(b1), "0"(c0), "1"(c1));
+#else
+    asm volatile("\n \
+            v_mac_f32 %0, %2, %3 \n \
+            v_mac_f32 %1, %2, %4 \n \
+            "
+                 : "=v"(c0), "=v"(c1)
+                 : "v"(a), "v"(b0), "v"(b1), "0"(c0), "1"(c1));
+#endif
 }
 
 // c0 += inner_product(a, b0)
@@ -30,6 +42,7 @@
 __device__ void amd_assembly_outer_product_1x4(
     float a, float b0, float b1, float b2, float b3, float& c0, float& c1, float& c2, float& c3)
 {
+#if defined(CK_USE_AMD_V_FMAC_F32)
     asm volatile("\n \
             v_fmac_f32 %0, %4, %5 \n \
             v_fmac_f32 %1, %4, %6 \n \
@@ -38,6 +48,16 @@
             "
                  : "=v"(c0), "=v"(c1), "=v"(c2), "=v"(c3)
                  : "v"(a), "v"(b0), "v"(b1), "v"(b2), "v"(b3), "0"(c0), "1"(c1), "2"(c2), "3"(c3));
+#else
+    asm volatile("\n \
+            v_mac_f32 %0, %4, %5 \n \
+            v_mac_f32 %1, %4, %6 \n \
+            v_mac_f32 %2, %4, %7 \n \
+            v_mac_f32 %3, %4, %8 \n \
+            "
+                 : "=v"(c0), "=v"(c1), "=v"(c2), "=v"(c3)
+                 : "v"(a), "v"(b0), "v"(b1), "v"(b2), "v"(b3), "0"(c0), "1"(c1), "2"(c2), "3"(c3));
+#endif
 }
 
 // c0 += inner_product(a, b0)
 
@@ -65,12 +68,27 @@
 __device__ void
 amd_assembly_outer_product_1x2(half2_t a, half2_t b0, half2_t b1, float& c0, float& c1)
 {
+#if defined(CK_USE_AMD_V_DOT2_F32_F16)
     asm volatile("\n \
             v_dot2_f32_f16 %0, %2, %3, %0\n \
             v_dot2_f32_f16 %1, %2, %4, %1\n \
             "
                  : "=v"(c0), "=v"(c1)
                  : "v"(a), "v"(b0), "v"(b1), "0"(c0), "1"(c1));
+#else
+    const vector_type<half_t, 2> a_vector{a};
+    const vector_type<half_t, 2> b0_vector{b0};
+    const vector_type<half_t, 2> b1_vector{b1};
+
+    #pragma omp parallel for reduction(+:c0,c1)
+    static_for<0, 2, 1>{}([&](auto i) {
+        c0 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b0_vector.AsType<half_t>()[i]);
+
+        c1 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b1_vector.AsType<half_t>()[i]);
+    });
+#endif
 }
 
 // c0 += inner_product(a, b0)
@@ -78,6 +96,7 @@
 __device__ void
 amd_assembly_outer_product_1x2(half4_t a, half4_t b0, half4_t b1, float& c0, float& c1)
 {
+#if defined(CK_USE_AMD_V_DOT2_F32_F16)
     // TODO remove pointer casting
     const half2_t* p_a_half2  = c_style_pointer_cast<const half2_t*>(&a);
     const half2_t* p_b0_half2 = c_style_pointer_cast<const half2_t*>(&b0);
@@ -99,6 +118,20 @@
                    "v"(p_b1_half2[1]),
                    "0"(c0),
                    "1"(c1));
+#else
+    const vector_type<half_t, 4> a_vector{a};
+    const vector_type<half_t, 4> b0_vector{b0};
+    const vector_type<half_t, 4> b1_vector{b1};
+
+    #pragma omp parallel for reduction(+:c0,c1)
+    static_for<0, 4, 1>{}([&](auto i) {
+        c0 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b0_vector.AsType<half_t>()[i]);
+
+        c1 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b1_vector.AsType<half_t>()[i]);
+    });
+#endif
 }
 
 // c0 += inner_product(a, b0)
@@ -115,6 +148,7 @@
                                                float& c2,
                                                float& c3)
 {
+#if defined(CK_USE_AMD_V_DOT2_F32_F16)
     asm volatile("\n \
             v_dot2_f32_f16 %0, %4, %5, %0\n \
             v_dot2_f32_f16 %1, %4, %6, %1\n \
@@ -123,6 +157,28 @@
             "
                  : "=v"(c0), "=v"(c1), "=v"(c2), "=v"(c3)
                  : "v"(a), "v"(b0), "v"(b1), "v"(b2), "v"(b3), "0"(c0), "1"(c1), "2"(c2), "3"(c3));
+#else
+    const vector_type<half_t, 2> a_vector{a};
+    const vector_type<half_t, 2> b0_vector{b0};
+    const vector_type<half_t, 2> b1_vector{b1};
+    const vector_type<half_t, 2> b2_vector{b2};
+    const vector_type<half_t, 2> b3_vector{b3};
+
+    #pragma omp parallel for reduction(+:c0,c1,c2,c3)
+    static_for<0, 2, 1>{}([&](auto i) {
+        c0 += type_convert<int32_t>(a_vector.AsType<half _t>()[i]) *
+             type_convert<int32_t>(b0_vector.AsType<half_t>()[i]);
+
+        c1 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b1_vector.AsType<half_t>()[i]);
+
+        c2 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b2_vector.AsType<half_t>()[i]);
+
+        c3 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b3_vector.AsType<half_t>()[i]);
+    });
+#endif
 }
 
 // c0 += inner_product(a, b0)
@@ -140,6 +196,7 @@
                                                float& c3)
 {
     // TODO remove pointer casting
+#if defined(CK_USE_AMD_V_DOT2_F32_F16)
     const half2_t* p_a_half2  = c_style_pointer_cast<const half2_t*>(&a);
     const half2_t* p_b0_half2 = c_style_pointer_cast<const half2_t*>(&b0);
     const half2_t* p_b1_half2 = c_style_pointer_cast<const half2_t*>(&b1);
@@ -172,6 +229,28 @@
                    "1"(c1),
                    "2"(c2),
                    "3"(c3));
+#else
+    const vector_type<half_t, 4> a_vector{a};
+    const vector_type<half_t, 4> b0_vector{b0};
+    const vector_type<half_t, 4> b1_vector{b1};
+    const vector_type<half_t, 4> b2_vector{b2};
+    const vector_type<half_t, 4> b3_vector{b3};
+
+    #pragma omp parallel for reduction(+:c0,c1,c2,c3)
+    static_for<0, 4, 1>{}([&](auto i) {
+        c0 += type_convert<int32_t>(a_vector.AsType<half _t>()[i]) *
+             type_convert<int32_t>(b0_vector.AsType<half_t>()[i]);
+
+        c1 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b1_vector.AsType<half_t>()[i]);
+
+        c2 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b2_vector.AsType<half_t>()[i]);
+
+        c3 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b3_vector.AsType<half_t>()[i]);
+    });
+#endif
 }
 
 __device__ void amd_assembly_outer_product_1x4(half8_t a,
@@ -228,6 +307,7 @@
 __device__ void
 amd_assembly_outer_product_1x2(int8x4_t a, int8x4_t b0, int8x4_t b1, int32_t& c0, int32_t& c1)
 {
+#if defined(CK_USE_AMD_V_DOT4_I32_I8)
 #if 1
     asm volatile("\n \
             v_dot4_i32_i8 %0, %2, %3, %0\n \
@@ -243,6 +323,17 @@
     c0     = __builtin_amdgcn_sdot4(bit_cast<int32_t>(a), bit_cast<int32_t>(b0), c0, false);
     c1     = __builtin_amdgcn_sdot4(bit_cast<int32_t>(a), bit_cast<int32_t>(b1), c1, false);
 #endif
+#else
+    const vector_type<int8_t, 4> a_vector{a};
+    const vector_type<int8_t, 4> b0_vector{b0};
+    const vector_type<int8_t, 4> b1_vector{b1};
+
+    #pragma omp parallel for reduction(+:c0,c1)
+    static_for<0, 4, 1>{}([&](auto i) {
+        c0 += static_cast<int32_t>(a_vector[i]) * static_cast<int32_t>(b0_vector[i]);
+        c1 += static_cast<int32_t>(a_vector[i]) * static_cast<int32_t>(b1_vector[i]);
+    });
+#endif
 }
 
 // c0 += inner_product(a, b0)
@@ -259,6 +350,7 @@
                                                int32_t& c2,
                                                int32_t& c3)
 {
+#if defined(CK_USE_AMD_V_DOT4_I32_I8)
 #if 1
     asm volatile("\n \
             v_dot4_i32_i8 %0, %4, %5, %0\n \
@@ -282,6 +374,21 @@
     c2     = __builtin_amdgcn_sdot4(bit_cast<int32_t>(a), bit_cast<int32_t>(b2), c2, false);
     c3     = __builtin_amdgcn_sdot4(bit_cast<int32_t>(a), bit_cast<int32_t>(b3), c3, false);
 #endif
+#else
+    const vector_type<int8_t, 4> a_vector{a};
+    const vector_type<int8_t, 4> b0_vector{b0};
+    const vector_type<int8_t, 4> b1_vector{b1};
+    const vector_type<int8_t, 4> b2_vector{b2};
+    const vector_type<int8_t, 4> b3_vector{b3};
+
+    #pragma omp parallel for reduction(+:c0,c1,c2,c3)
+    static_for<0, 4, 1>{}([&](auto i) {
+        c0 += static_cast<int32_t>(a_vector[i]) * static_cast<int32_t>(b0_vector[i]);
+        c1 += static_cast<int32_t>(a_vector[i]) * static_cast<int32_t>(b1_vector[i]);
+        c2 += static_cast<int32_t>(a_vector[i]) * static_cast<int32_t>(b2_vector[i]);
+        c3 += static_cast<int32_t>(a_vector[i]) * static_cast<int32_t>(b3_vector[i]);
+    });
+#endif
 }
 
 __device__ void amd_assembly_outer_product_1x4(int8x8_t a,
