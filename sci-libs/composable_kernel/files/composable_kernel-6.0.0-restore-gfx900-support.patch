--- ./include/ck/ck.hpp.orig	2023-12-07 18:33:34.435503872 +0000
+++ ./include/ck/ck.hpp	2023-12-07 18:45:11.005502374 +0000
@@ -30,8 +30,8 @@
 // buffer resource
 #ifndef __HIP_DEVICE_COMPILE__ // for host code
 #define CK_BUFFER_RESOURCE_3RD_DWORD -1
-#elif defined(__gfx803__) || defined(__gfx900__) || defined(__gfx906__) || defined(__gfx908__) || \
-    defined(__gfx90a__) || defined(__gfx940__) || defined(__gfx941__) ||                          \
+#elif defined(__gfx803__) || defined(__gfx900__) || defined(__gfx902__) || defined(__gfx906__) || \
+    defined(__gfx908__) || defined(__gfx90a__) || defined(__gfx940__) || defined(__gfx941__) || \
     defined(__gfx942__) // for GPU code
 #define CK_BUFFER_RESOURCE_3RD_DWORD 0x00020000
 #elif defined(__gfx1030__) // for GPU code
@@ -41,7 +41,7 @@
 
 // FMA instruction
 #ifndef __HIP_DEVICE_COMPILE__                   // for host code, define nothing
-#elif defined(__gfx803__) || defined(__gfx900__) // for GPU code
+#elif defined(__gfx803__) || defined(__gfx900__) || defined(__gfx902__) // for GPU code
 #define CK_USE_AMD_V_MAC_F32
 #elif defined(__gfx906__) || defined(__gfx908__) || defined(__gfx90a__) || defined(__gfx1030__) || \
     defined(__gfx940__) // for GPU code
--- ./include/ck/utility/amd_inline_asm.hpp.orig	2023-04-29 00:22:59.000000000 +0100
+++ ./include/ck/utility/amd_inline_asm.hpp	2023-12-08 13:57:14.553213052 +0000
@@ -15,12 +15,21 @@
 // c1 += inner_product(a, b1)
 __device__ void amd_assembly_outer_product_1x2(float a, float b0, float b1, float& c0, float& c1)
 {
+#if defined(CK_USE_AMD_V_FMAC_F32)
     asm volatile("\n \
             v_fmac_f32 %0, %2, %3 \n \
             v_fmac_f32 %1, %2, %4 \n \
             "
                  : "=v"(c0), "=v"(c1)
                  : "v"(a), "v"(b0), "v"(b1), "0"(c0), "1"(c1));
+#else
+    asm volatile("\n \
+            v_mac_f32 %0, %2, %3 \n \
+            v_mac_f32 %1, %2, %4 \n \
+            "
+                 : "=v"(c0), "=v"(c1)
+                 : "v"(a), "v"(b0), "v"(b1), "0"(c0), "1"(c1));
+#endif
 }
 
 // c0 += inner_product(a, b0)
@@ -30,6 +39,7 @@
 __device__ void amd_assembly_outer_product_1x4(
     float a, float b0, float b1, float b2, float b3, float& c0, float& c1, float& c2, float& c3)
 {
+#if defined(CK_USE_AMD_V_FMAC_F32)
     asm volatile("\n \
             v_fmac_f32 %0, %4, %5 \n \
             v_fmac_f32 %1, %4, %6 \n \
@@ -38,6 +48,16 @@
             "
                  : "=v"(c0), "=v"(c1), "=v"(c2), "=v"(c3)
                  : "v"(a), "v"(b0), "v"(b1), "v"(b2), "v"(b3), "0"(c0), "1"(c1), "2"(c2), "3"(c3));
+#else
+    asm volatile("\n \
+            v_mac_f32 %0, %4, %5 \n \
+            v_mac_f32 %1, %4, %6 \n \
+            v_mac_f32 %2, %4, %7 \n \
+            v_mac_f32 %3, %4, %8 \n \
+            "
+                 : "=v"(c0), "=v"(c1), "=v"(c2), "=v"(c3)
+                 : "v"(a), "v"(b0), "v"(b1), "v"(b2), "v"(b3), "0"(c0), "1"(c1), "2"(c2), "3"(c3));
+#endif
 }
 
 // c0 += inner_product(a, b0)
@@ -45,12 +65,26 @@
 __device__ void
 amd_assembly_outer_product_1x2(half2_t a, half2_t b0, half2_t b1, float& c0, float& c1)
 {
+#if defined(CK_USE_AMD_V_DOT2_F32_F16)
     asm volatile("\n \
             v_dot2_f32_f16 %0, %2, %3, %0\n \
             v_dot2_f32_f16 %1, %2, %4, %1\n \
             "
                  : "=v"(c0), "=v"(c1)
                  : "v"(a), "v"(b0), "v"(b1), "0"(c0), "1"(c1));
+#else
+    const vector_type<half_t, 2> a_vector{a};
+    const vector_type<half_t, 2> b0_vector{b0};
+    const vector_type<half_t, 2> b1_vector{b1};
+
+    static_for<0, 2, 1>{}([&](auto i) {
+        c0 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b0_vector.AsType<half_t>()[i]);
+
+        c1 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b1_vector.AsType<half_t>()[i]);
+    });
+#endif
 }
 
 // c0 += inner_product(a, b0)
@@ -58,6 +92,7 @@
 __device__ void
 amd_assembly_outer_product_1x2(half4_t a, half4_t b0, half4_t b1, float& c0, float& c1)
 {
+#if defined(CK_USE_AMD_V_DOT2_F32_F16)
     // TODO remove pointer casting
     const half2_t* p_a_half2  = c_style_pointer_cast<const half2_t*>(&a);
     const half2_t* p_b0_half2 = c_style_pointer_cast<const half2_t*>(&b0);
@@ -79,6 +114,19 @@
                    "v"(p_b1_half2[1]),
                    "0"(c0),
                    "1"(c1));
+#else
+    const vector_type<half_t, 4> a_vector{a};
+    const vector_type<half_t, 4> b0_vector{b0};
+    const vector_type<half_t, 4> b1_vector{b1};
+
+    static_for<0, 4, 1>{}([&](auto i) {
+        c0 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b0_vector.AsType<half_t>()[i]);
+
+        c1 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b1_vector.AsType<half_t>()[i]);
+    });
+#endif
 }
 
 // c0 += inner_product(a, b0)
@@ -95,6 +143,7 @@
                                                float& c2,
                                                float& c3)
 {
+#if defined(CK_USE_AMD_V_DOT2_F32_F16)
     asm volatile("\n \
             v_dot2_f32_f16 %0, %4, %5, %0\n \
             v_dot2_f32_f16 %1, %4, %6, %1\n \
@@ -103,6 +152,27 @@
             "
                  : "=v"(c0), "=v"(c1), "=v"(c2), "=v"(c3)
                  : "v"(a), "v"(b0), "v"(b1), "v"(b2), "v"(b3), "0"(c0), "1"(c1), "2"(c2), "3"(c3));
+#else
+    const vector_type<half_t, 2> a_vector{a};
+    const vector_type<half_t, 2> b0_vector{b0};
+    const vector_type<half_t, 2> b1_vector{b1};
+    const vector_type<half_t, 2> b2_vector{b2};
+    const vector_type<half_t, 2> b3_vector{b3};
+
+    static_for<0, 2, 1>{}([&](auto i) {
+        c0 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b0_vector.AsType<half_t>()[i]);
+
+        c1 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b1_vector.AsType<half_t>()[i]);
+
+        c2 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b2_vector.AsType<half_t>()[i]);
+
+        c3 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b3_vector.AsType<half_t>()[i]);
+    });
+#endif
 }
 
 // c0 += inner_product(a, b0)
@@ -119,6 +189,7 @@
                                                float& c2,
                                                float& c3)
 {
+#if defined(CK_USE_AMD_V_DOT2_F32_F16)
     // TODO remove pointer casting
     const half2_t* p_a_half2  = c_style_pointer_cast<const half2_t*>(&a);
     const half2_t* p_b0_half2 = c_style_pointer_cast<const half2_t*>(&b0);
@@ -152,6 +223,27 @@
                    "1"(c1),
                    "2"(c2),
                    "3"(c3));
+#else
+    const vector_type<half_t, 4> a_vector{a};
+    const vector_type<half_t, 4> b0_vector{b0};
+    const vector_type<half_t, 4> b1_vector{b1};
+    const vector_type<half_t, 4> b2_vector{b2};
+    const vector_type<half_t, 4> b3_vector{b3};
+
+    static_for<0, 4, 1>{}([&](auto i) {
+        c0 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b0_vector.AsType<half_t>()[i]);
+
+        c1 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b1_vector.AsType<half_t>()[i]);
+
+        c2 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b2_vector.AsType<half_t>()[i]);
+
+        c3 += type_convert<int32_t>(a_vector.AsType<half_t>()[i]) *
+             type_convert<int32_t>(b3_vector.AsType<half_t>()[i]);
+    });
+#endif
 }
 
 __device__ void amd_assembly_outer_product_1x4(half8_t a,
@@ -208,6 +300,7 @@
 __device__ void
 amd_assembly_outer_product_1x2(int8x4_t a, int8x4_t b0, int8x4_t b1, int32_t& c0, int32_t& c1)
 {
+#if defined(CK_USE_AMD_V_DOT4_I32_I8)
 #if 1
     asm volatile("\n \
             v_dot4_i32_i8 %0, %2, %3, %0\n \
@@ -223,6 +316,18 @@
     c0     = __builtin_amdgcn_sdot4(bit_cast<int32_t>(a), bit_cast<int32_t>(b0), c0, false);
     c1     = __builtin_amdgcn_sdot4(bit_cast<int32_t>(a), bit_cast<int32_t>(b1), c1, false);
 #endif
+#else
+    const vector_type<int8_t, 4> a_vector{a};
+    const vector_type<int8_t, 4> b0_vector{b0};
+    const vector_type<int8_t, 4> b1_vector{b1};
+
+    static_for<0, 4, 1>{}([&](auto i) {
+        c0 += type_convert<int32_t>(a_vector.AsType<int8_t>()[i]) *
+             type_convert<int32_t>(b0_vector.AsType<int8_t>()[i]);
+        c1 += type_convert<int32_t>(a_vector.AsType<int8_t>()[i]) *
+             type_convert<int32_t>(b1_vector.AsType<int8_t>()[i]);
+    });
+#endif
 }
 
 // c0 += inner_product(a, b0)
@@ -239,6 +342,7 @@
                                                int32_t& c2,
                                                int32_t& c3)
 {
+#if defined(CK_USE_AMD_V_DOT4_I32_I8)
 #if 1
     asm volatile("\n \
             v_dot4_i32_i8 %0, %4, %5, %0\n \
@@ -262,6 +368,24 @@
     c2     = __builtin_amdgcn_sdot4(bit_cast<int32_t>(a), bit_cast<int32_t>(b2), c2, false);
     c3     = __builtin_amdgcn_sdot4(bit_cast<int32_t>(a), bit_cast<int32_t>(b3), c3, false);
 #endif
+#else
+    const vector_type<int8_t, 4> a_vector{a};
+    const vector_type<int8_t, 4> b0_vector{b0};
+    const vector_type<int8_t, 4> b1_vector{b1};
+    const vector_type<int8_t, 4> b2_vector{b2};
+    const vector_type<int8_t, 4> b3_vector{b3};
+
+    static_for<0, 4, 1>{}([&](auto i) {
+        c0 += type_convert<int32_t>(a_vector.AsType<int8_t>()[i]) *
+             type_convert<int32_t>(b0_vector.AsType<int8_t>()[i]);
+        c1 += type_convert<int32_t>(a_vector.AsType<int8_t>()[i]) *
+             type_convert<int32_t>(b1_vector.AsType<int8_t>()[i]);
+        c2 += type_convert<int32_t>(a_vector.AsType<int8_t>()[i]) *
+             type_convert<int32_t>(b2_vector.AsType<int8_t>()[i]);
+        c3 += type_convert<int32_t>(a_vector.AsType<int8_t>()[i]) *
+             type_convert<int32_t>(b3_vector.AsType<int8_t>()[i]);
+    });
+#endif
 }
 
 __device__ void amd_assembly_outer_product_1x4(int8x8_t a,
--- ./CMakeLists.txt~	2024-01-22 16:14:18.000000000 +0000
+++ ./CMakeLists.txt	2024-01-22 16:26:58.896817751 +0000
@@ -104,7 +104,7 @@
 #Setting GPU_TARGETS on command line will override this list
 if(NOT PROFILER_ONLY)
     rocm_check_target_ids(DEFAULT_GPU_TARGETS
-        TARGETS "gfx908;gfx90a;gfx940;gfx941;gfx942;gfx1030;gfx1100;gfx1101;gfx1102")
+        TARGETS "gfx900;gfx902;gfx906;gfx908;gfx90a;gfx940;gfx941;gfx942;gfx1030;gfx1100;gfx1101;gfx1102")
 else()
     add_definitions(-DPROFILER_ONLY)
     set(GPU_TARGETS "" CACHE STRING "" FORCE)
@@ -112,7 +112,7 @@
         message(FATAL_ERROR "For PROFILE_ONLY build, please do not set GPU_TARGETS, use GPU_ARCH = gfx90, gfx94, gfx10, or gfx11")
     endif()
     if(GPU_ARCH MATCHES "gfx90")
-        rocm_check_target_ids(DEFAULT_GPU_TARGETS TARGETS "gfx908;gfx90a")
+        rocm_check_target_ids(DEFAULT_GPU_TARGETS TARGETS "gfx900;gfx902;gfx906;gfx908;gfx90a")
     elseif(GPU_ARCH MATCHES "gfx94")
         rocm_check_target_ids(DEFAULT_GPU_TARGETS TARGETS "gfx940;gfx941;gfx942")
     elseif(GPU_ARCH MATCHES "gfx10")
--- ./include/ck/tensor_operation/gpu/device/impl/device_batched_gemm_multiple_d_dl.hpp~	2023-11-08 23:03:57.000000000 +0000
+++ ./include/ck/tensor_operation/gpu/device/impl/device_batched_gemm_multiple_d_dl.hpp	2024-01-22 21:53:20.719658611 +0000
@@ -70,9 +70,9 @@
             const ComputePtrOffsetOfBatch compute_ptr_offset_of_batch,
             const Block2CTileMap block_2_ctile_map)
 {
-#if(!defined(__HIP_DEVICE_COMPILE__) || defined(__gfx906__) || defined(__gfx908__) ||             \
-    defined(__gfx90a__) || defined(__gfx940__) || defined(__gfx1030__) || defined(__gfx1100__) || \
-    defined(__gfx1101__) || defined(__gfx1102__))
+#if(!defined(__HIP_DEVICE_COMPILE__) || defined(__gfx900__) || defined(__gfx902__) ||           \
+    defined(__gfx906__) || defined(__gfx908__) || defined(__gfx90a__) || defined(__gfx940__) || \
+    defined(__gfx1030__) || defined(__gfx1100__) || defined(__gfx1101__) || defined(__gfx1102__))
 
     const index_t num_blocks_per_batch =
         __builtin_amdgcn_readfirstlane(get_grid_size() / batch_count);
@@ -649,7 +649,8 @@
     static bool IsSupportedArgument(const Argument& arg)
     {
         // TODO: Enable for gfx90a after complier fix
-        if(ck::get_device_name() == "gfx906" || ck::get_device_name() == "gfx90a" ||
+        if(ck::get_device_name() == "gfx900" || ck::get_device_name() == "gfx902" ||
+           ck::get_device_name() == "gfx906" || ck::get_device_name() == "gfx90a" ||
            ck::get_device_name() == "gfx908" || ck::get_device_name() == "gfx1030" ||
            ck::get_device_name() == "gfx940" || ck::get_device_name() == "gfx1100" ||
            ck::get_device_name() == "gfx1101" || ck::get_device_name() == "gfx1102")
--- ./include/ck/tensor_operation/gpu/device/impl/device_convnd_bwd_data_nwc_kxc_nwk_dl.hpp~	2023-11-08 23:03:57.000000000 +0000
+++ ./include/ck/tensor_operation/gpu/device/impl/device_convnd_bwd_data_nwc_kxc_nwk_dl.hpp	2024-01-22 21:54:22.579657148 +0000
@@ -1393,7 +1393,8 @@
     static bool IsSupportedArgument(const Argument& arg)
     {
         // check device
-        if(!(ck::get_device_name() == "gfx906" || ck::get_device_name() == "gfx1030" ||
+        if(!(ck::get_device_name() == "gfx900" || ck::get_device_name() == "gfx902" ||
+             ck::get_device_name() == "gfx906" || ck::get_device_name() == "gfx1030" ||
              ck::get_device_name() == "gfx1100" || ck::get_device_name() == "gfx1101" ||
              ck::get_device_name() == "gfx1102"))
         {
--- ./include/ck/tensor_operation/gpu/device/impl/device_gemm_dl.hpp~	2023-11-08 23:03:57.000000000 +0000
+++ ./include/ck/tensor_operation/gpu/device/impl/device_gemm_dl.hpp	2024-01-22 21:55:03.902989496 +0000
@@ -536,7 +536,8 @@
             }
         }
 
-        if(ck::get_device_name() == "gfx906" || ck::get_device_name() == "gfx1030" ||
+        if(ck::get_device_name() == "gfx900" || ck::get_device_name() == "gfx902" ||
+           ck::get_device_name() == "gfx906" || ck::get_device_name() == "gfx1030" ||
            ck::get_device_name() == "gfx1100" || ck::get_device_name() == "gfx1101" ||
            ck::get_device_name() == "gfx1102")
         {
--- ./include/ck/tensor_operation/gpu/device/impl/device_gemm_multiple_d_dl.hpp~	2023-11-08 23:03:57.000000000 +0000
+++ ./include/ck/tensor_operation/gpu/device/impl/device_gemm_multiple_d_dl.hpp	2024-01-22 21:57:35.462985952 +0000
@@ -50,9 +50,10 @@
             const CGridDesc_M0_M10_M11_N0_N10_N11 e_grid_desc_m0_m10_m11_n0_n10_n11,
             const Block2CTileMap block_2_ctile_map)
 {
-#if(!defined(__HIP_DEVICE_COMPILE__) || defined(__gfx906__) || defined(__gfx908__) ||             \
-    defined(__gfx90a__) || defined(__gfx940__) || defined(__gfx1030__) || defined(__gfx1100__) || \
-    defined(__gfx1101__) || defined(__gfx1102__) || defined(__gfx941__) || defined(__gfx942__))
+#if(!defined(__HIP_DEVICE_COMPILE__) || defined(__gfx900__) || defined(__gfx902__) || \
+    defined(__gfx906__) || defined(__gfx906__)  || defined(__gfx90a__) || defined(__gfx940__) || \
+    defined(__gfx1030__) || defined(__gfx1100__) || defined(__gfx1101__) || \
+    defined(__gfx1102__) || defined(__gfx941__) || defined(__gfx942__))
 
     constexpr index_t shared_block_size =
         GridwiseGemm::GetSharedMemoryNumberOfByte() / sizeof(ABDataType);
@@ -552,7 +553,8 @@
 
     static bool IsSupportedArgument(const Argument& arg)
     {
-        if(ck::get_device_name() == "gfx906" || ck::get_device_name() == "gfx908" ||
+        if(ck::get_device_name() == "gfx900" || ck::get_device_name() == "gfx902" ||
+           ck::get_device_name() == "gfx906" || ck::get_device_name() == "gfx908" ||
            ck::get_device_name() == "gfx90a" || ck::get_device_name() == "gfx1030" ||
            ck::get_device_name() == "gfx940" || ck::get_device_name() == "gfx1100" ||
            ck::get_device_name() == "gfx1101" || ck::get_device_name() == "gfx1102" ||
--- ./include/ck/tensor_operation/gpu/device/impl/device_grouped_conv_bwd_weight_dl.hpp~	2023-11-08 23:03:57.000000000 +0000
+++ ./include/ck/tensor_operation/gpu/device/impl/device_grouped_conv_bwd_weight_dl.hpp	2024-01-22 21:58:52.142984162 +0000
@@ -73,9 +73,10 @@
             const Block2CTileMap block_2_ctile_map,
             const ComputePtrOffsetOfBatch compute_ptr_offset_of_batch)
 {
-#if(!defined(__HIP_DEVICE_COMPILE__) || defined(__gfx906__) || defined(__gfx1030__) ||           \
-    defined(__gfx90a__) || defined(__gfx908__) || defined(__gfx940__) || defined(__gfx1100__) || \
-    defined(__gfx1101__) || defined(__gfx1102__) || defined(__gfx941__) || defined(__gfx942__))
+#if(!defined(__HIP_DEVICE_COMPILE__) || defined(__gfx900__) || defined(__gfx902__) ||            \
+    defined(__gfx906__) || defined(__gfx1030__) || defined(__gfx90a__) || defined(__gfx908__) ||    \
+    defined(__gfx940__) || defined(__gfx1100__) || defined(__gfx1101__) ||                       \
+    defined(__gfx1102__) || defined(__gfx941__) || defined(__gfx942__))
     const index_t num_blocks_per_batch =
         __builtin_amdgcn_readfirstlane(get_grid_size() / batch_count);
     const index_t g_idx = __builtin_amdgcn_readfirstlane(get_block_1d_id() / num_blocks_per_batch);
--- ./include/ck/tensor_operation/gpu/device/impl/device_grouped_conv_fwd_dl_multiple_d_nhwc_kyxc_nhwk.hpp~	2023-11-08 23:03:57.000000000 +0000
+++ ./include/ck/tensor_operation/gpu/device/impl/device_grouped_conv_fwd_dl_multiple_d_nhwc_kyxc_nhwk.hpp	2024-01-22 22:00:54.646314575 +0000
@@ -90,8 +90,9 @@
             const Block2CTileMap block_2_ctile_map,
             const ComputePtrOffsetOfBatch compute_ptr_offset_of_batch)
 {
-#if(!defined(__HIP_DEVICE_COMPILE__) || defined(__gfx906__) || defined(__gfx1030__) ||           \
-    defined(__gfx90a__) || defined(__gfx908__) || defined(__gfx940__) || defined(__gfx1100__) || \
+#if(!defined(__HIP_DEVICE_COMPILE__) || defined(__gfx900__) || defined(__gfx902__) ||            \
+    defined(__gfx906__) || defined(__gfx1030__)  || defined(__gfx90a__) ||                       \
+    defined(__gfx908__) || defined(__gfx940__) || defined(__gfx1100__) ||                        \
     defined(__gfx1101__) || defined(__gfx1102__) || defined(__gfx941__) || defined(__gfx942__))
     // offset base pointer for each work-group
     const index_t num_blocks_per_batch =
@@ -666,7 +667,8 @@
         namespace ctc = tensor_layout::convolution;
 
         // check device
-        if(!(ck::get_device_name() == "gfx906" || ck::get_device_name() == "gfx1030" ||
+        if(!(ck::get_device_name() == "gfx900" || ck::get_device_name() == "gfx902" ||
+             ck::get_device_name() == "gfx906" || ck::get_device_name() == "gfx1030" ||
              ck::get_device_name() == "gfx90a" || ck::get_device_name() == "gfx908" ||
              ck::get_device_name() == "gfx940" || ck::get_device_name() == "gfx1100" ||
              ck::get_device_name() == "gfx1101" || ck::get_device_name() == "gfx1102" ||
--- ./include/ck/tensor_operation/gpu/device/impl/device_grouped_gemm_multiple_d_dl.hpp~	2023-11-08 23:03:57.000000000 +0000
+++ ./include/ck/tensor_operation/gpu/device/impl/device_grouped_gemm_multiple_d_dl.hpp	2024-01-22 22:04:34.352976163 +0000
@@ -39,9 +39,10 @@
                                           const BElementwiseOperation b_element_op,
                                           const CDEElementwiseOperation cde_element_op)
 {
-#if(!defined(__HIP_DEVICE_COMPILE__) || defined(__gfx906__) || defined(__gfx908__) ||              \
-    defined(__gfx90a__) || defined(__gfx1030__) || defined(__gfx1100__) || defined(__gfx1101__) || \
-    defined(__gfx1102__) || defined(__gfx940__) || defined(__gfx941__) || defined(__gfx942__))
+#if(!defined(__HIP_DEVICE_COMPILE__) || defined(__gfx900__) || defined(__gfx902__) ||              \
+    defined(__gfx906__) || defined(__gfx908__) || defined(__gfx90a__) || defined(__gfx1030__) ||   \
+    defined(__gfx1100__) || defined(__gfx1101__) || defined(__gfx1102__) || defined(__gfx940__) || \
+    defined(__gfx941__) || defined(__gfx942__))
     __shared__ char p_shared[GridwiseGemm::GetSharedMemoryNumberOfByte()];
 
     const index_t block_id = get_block_1d_id();
@@ -671,8 +672,8 @@
         const std::string device_name = ck::get_device_name();
 
         //  TODO add newer Navi arch
-        if(device_name != "gfx906" and device_name != "gfx908" and device_name != "gfx90a" and
-           device_name != "gfx1030")
+        if(device_name != "gfx900" and device_name != "gfx902" and device_name != "gfx906" and
+           device_name != "gfx908" and device_name != "gfx90a" and device_name != "gfx1030")
         {
             return false;
         }
--- ./include/ck/tensor_operation/gpu/grid/gridwise_tensor_rearrange.hpp~	2023-11-08 23:03:57.000000000 +0000
+++ ./include/ck/tensor_operation/gpu/grid/gridwise_tensor_rearrange.hpp	2024-01-22 22:05:45.886307816 +0000
@@ -32,9 +32,10 @@
                                 OutputDataType* __restrict__ p_out_global,
                                 const Block2ETileMap block_2_tile_map)
 {
-#if(!defined(__HIP_DEVICE_COMPILE__) || defined(__gfx906__) || defined(__gfx908__) ||             \
-    defined(__gfx90a__) || defined(__gfx940__) || defined(__gfx1030__) || defined(__gfx1100__) || \
-    defined(__gfx1101__) || defined(__gfx1102__) || defined(__gfx941__) || defined(__gfx942__))
+#if(!defined(__HIP_DEVICE_COMPILE__) || defined(__gfx900__) || defined(__gfx902__) ||             \
+    defined(__gfx906__) || defined(__gfx908__)|| defined(__gfx90a__) || defined(__gfx940__) ||    \
+    defined(__gfx1030__) || defined(__gfx1100__) || defined(__gfx1101__) ||                       \
+    defined(__gfx1102__) || defined(__gfx941__) || defined(__gfx942__))
     GridwiseTensorRearrangeKernel::Run(
         in_grid_desc, p_in_global, out_grid_desc, p_out_global, block_2_tile_map);
 #else
